{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f385ff62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c263e3879d42eab837f58d4ce8b8fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: what is an apple?\n",
      "0.43719715 apple is an fruit\n",
      "0.3098194 you can buy in Apple store\n",
      "Query: where can i get an IPhone?\n",
      "0.37321442 you can buy in Apple store\n",
      "0.28367645 apple is an fruit\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(\"Snowflake/snowflake-arctic-embed-m\")\n",
    "\n",
    "queries = ['what is an apple?', 'where can i get an IPhone?']\n",
    "documents = ['apple is an fruit', 'you can buy in Apple store']\n",
    "\n",
    "query_embeddings = embedding_model.encode(queries, prompt_name=\"query\", batch_size=1, show_progress_bar=True)\n",
    "document_embeddings = embedding_model.encode(documents)\n",
    "\n",
    "scores = query_embeddings @ document_embeddings.T\n",
    "for query, query_scores in zip(queries, scores):\n",
    "    doc_score_pairs = list(zip(documents, query_scores))\n",
    "    doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"Query: {query}\")\n",
    "    for document, score in doc_score_pairs:\n",
    "        print(score, document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97089181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 768)\n"
     ]
    }
   ],
   "source": [
    "query_embed = embedding_model.encode(queries[0], prompt_name=\"query\", precision=\"float32\", normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7974b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Title: 0 & 1 Plot: A girl called Zero stands in a crowded street, her occupation an assassin. As she completes her tasks one after another, Zero has been feeling uneasy lately. As her code name suggests, she has nothing to prove her existence. She overlaps herself with the vanishing landscape and begins to wander in search of the meaning of her own existence. At the same time, a young man named One is also wandering the streets, and like Zero, he is an assassin. Surrounded by a sense of emptiness, he is unable to find the value of his own existence. One day, the two of them meet as if they were led together. They sense each other, and by touching each other, they regain their existence and humanity. However, the organization feels that contact between assassins is dangerous, and decides to dispose of them. They call in a highly skilled assassin. Their battle for their own existence begins now!'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../../data/prep/film_data.csv\", usecols=['title', 'title_plot', 'title_meta'])\n",
    "df[\"title_plot\"].iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0888a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length_plot</th>\n",
       "      <th>length_meta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>773275.000000</td>\n",
       "      <td>773275.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>315.563265</td>\n",
       "      <td>257.151840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>206.885012</td>\n",
       "      <td>59.725598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>38.000000</td>\n",
       "      <td>181.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>160.000000</td>\n",
       "      <td>209.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>255.000000</td>\n",
       "      <td>239.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>421.000000</td>\n",
       "      <td>292.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1170.000000</td>\n",
       "      <td>1154.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         length_plot    length_meta\n",
       "count  773275.000000  773275.000000\n",
       "mean      315.563265     257.151840\n",
       "std       206.885012      59.725598\n",
       "min        38.000000     181.000000\n",
       "25%       160.000000     209.000000\n",
       "50%       255.000000     239.000000\n",
       "75%       421.000000     292.000000\n",
       "max      1170.000000    1154.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df['length_plot'] = df['title_plot'].apply(lambda plot: len(plot))\n",
    "# df['length_meta'] = df['title_meta'].apply(lambda meta: len(meta))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad893946",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3801/1455609617.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\u001b[0m\u001b[0;34mMaximum_seq_length: \u001b[0m\u001b[0;34m{\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_max_seq_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\u001b[0m\u001b[0;34mEmbedding dims: \u001b[0m\u001b[0;34m{\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sentence_embedding_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embedding_model' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum_seq_length: {embedding_model.get_max_seq_length()}\")\n",
    "print(f\"Embedding dims: {embedding_model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a36e226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce9d90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Локально через llama.cpp\n",
    "# Установите: pip install llama-cpp-python huggingface-hub\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Download Mistral-7B-Instruct GGUF model (Q4_K_M quantization)\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n",
    "    filename=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n",
    ")\n",
    "print(f\"Model downloaded to: {model_path}\")\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    chat_format=\"mistral-instruct\",\n",
    "    n_ctx=2048,\n",
    "    n_threads=8,   # можно поставить = числу физических ядер CPU\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Recommend a good road movies.\"}]\n",
    "resp = llm.create_chat_completion(messages=messages)\n",
    "\n",
    "print(resp[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a69a84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a09ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a text generation pipeline with a small model\n",
    "generator1 = pipeline('text-generation', model='gpt2')\n",
    "# DistilGPT-2 (меньше GPT-2, быстрее)\n",
    "generator2 = pipeline('text-generation', model='distilgpt2')\n",
    "\n",
    "# DialoGPT-small (для диалогов)\n",
    "generator3 = pipeline('text-generation', model='microsoft/DialoGPT-small')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2e852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "prompt = \"reccomend me a good road movie\"\n",
    "result = generator1(prompt, max_length=50, num_return_sequences=1)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2da8272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Локально через llama.cpp\n",
    "# Установите: pip install llama-cpp-python huggingface-hub\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Download LLaMA-2 13B Chat GGUF model (Q4_K_M quantization)\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"TheBloke/Llama-2-13B-Chat-GGUF\",\n",
    "    filename=\"llama-2-13b-chat.Q4_K_M.gguf\"\n",
    ")\n",
    "print(f\"Model downloaded to: {model_path}\")\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    chat_format=\"llama-2\",\n",
    "    n_ctx=2048,\n",
    "    n_threads=8,   # можно поставить = числу физических ядер CPU\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Recommend a good road movie.\"}]\n",
    "resp = llm.create_chat_completion(messages=messages)\n",
    "\n",
    "print(resp[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e72c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Локально через llama.cpp\n",
    "# Установите: pip install llama-cpp-python huggingface-hub\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Download Mistral-7B-Instruct GGUF model (Q4_K_M quantization)\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n",
    "    filename=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n",
    ")\n",
    "print(f\"Model downloaded to: {model_path}\")\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    chat_format=\"mistral-instruct\",\n",
    "    n_ctx=2048,\n",
    "    n_threads=8,   # можно поставить = числу физических ядер CPU\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Recommend a good road movie.\"}]\n",
    "resp = llm.create_chat_completion(messages=messages)\n",
    "\n",
    "print(resp[\"choices\"][0][\"message\"][\"content\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Ops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
